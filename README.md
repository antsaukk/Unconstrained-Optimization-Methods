# Unconstrained-Optimization-Methods 

In this project, several unconstrained optimization methods were developed and tested. Namely, [Gradient](https://en.wikipedia.org/wiki/Gradient_descent),
[Gradient method with momentum](https://machinelearningmastery.com/gradient-descent-with-momentum-from-scratch/), [Newton's Method](https://en.wikipedia.org/wiki/Newton%27s_method),
and [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). 
For the choice of optimal step size, algorithms employed exact and inexact line search methods: [Bisection](https://en.wikipedia.org/wiki/Bisection_method) and
[Armijo rule](https://en.wikipedia.org/wiki/Wolfe_conditions#Armijo_rule_and_curvature). The project implemented in [Julia programming language](https://julialang.org/).
